{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from data_loader import load_data_1m\n",
    "from feature_calculations_2 import (\n",
    "    resample_data, calculate_MA_data, calculate_ema_bollinger_bands, calculate_rsi,\n",
    "    calculate_macd, calculate_stochastic_oscillator, calculate_adx, calculate_atr,\n",
    "    calculate_obv, calculate_williams_r, base_feature_fn, cyclic_encode_fn, calculate_support_resistance_numba\n",
    ")\n",
    "from strategies import BB_fitness_fn, BB_MACD_fitness_fn\n",
    "from dataset import make_dataset, replace_nan_with_zero\n",
    "from train_functions_bi_cul import inference, fitness_fn, generation_valid, generation_test\n",
    "\n",
    "from Prescriptor import Prescriptor, CryptoModelTCN\n",
    "from Evolution.crossover import UniformCrossover, WeightedSumCrossover, DifferentialEvolutionOperator, CenDE_DOBLOperator, SkipCrossover\n",
    "from Evolution.mutation import MultiplyNormalMutation, MultiplyUniformMutation, AddNormalMutation, AddUniformMutation, ChainMutation, FlipSignMutation\n",
    "from Evolution.mutation import RandomValueMutation\n",
    "from Evolution.selection import RouletteSelection, TournamentSelection, ParetoLexsortSelection\n",
    "from Evolution import Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_1m = load_data_1m('/root/daily/bit/data/1min_bitusdt.pkl')\n",
    "# data_1m = data_1m.iloc[:200000]\n",
    "\n",
    "# Resample data to 1D for daily data\n",
    "data_1d = resample_data(data_1m, '1D')\n",
    "data_1d['Close time'] = data_1d.index\n",
    "data_1d = data_1d.reset_index(drop=True)\n",
    "\n",
    "# 기존 사이클 인코딩 등 (필요에 따라 실행)\n",
    "data_1d, cyclice_encoding_1d = cyclic_encode_fn(data_1d, 'Close time', 'day_of_week')\n",
    "data_1m, cyclice_encoding = cyclic_encode_fn(data_1m, 'Open time')\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1분봉 데이터: 다양한 윈도우(예: 20, 60, 120, 240, 480봉)를 사용하여 feature 생성\n",
    "time_windows_1m = [20, 60, 120, 240, 480]\n",
    "features_1m = {}  # indicator별 컬럼명을 저장할 dictionary\n",
    "\n",
    "for window in time_windows_1m:\n",
    "    # 이동평균 (EMA)\n",
    "    data_1m, ma_cols, ma_cols_rel = calculate_MA_data(data_1m, window, 'EMA')\n",
    "    features_1m[f'MA_{window}'] = ma_cols_rel\n",
    "\n",
    "    # EMA Bollinger Bands\n",
    "    data_1m, bb_cols, bb_cols_rel = calculate_ema_bollinger_bands(data_1m, window)\n",
    "    features_1m[f'BB_{window}'] = bb_cols_rel\n",
    "\n",
    "    # RSI (윈도우 크기의 1/3 정도를 사용하거나 최소 5)\n",
    "    rsi_window = max(5, int(window / 3))\n",
    "    data_1m, rsi_cols = calculate_rsi(data_1m, window=rsi_window)\n",
    "    features_1m[f'RSI_{window}'] = rsi_cols\n",
    "\n",
    "    # MACD: 단기, 장기, 신호선 기간을 window에 비례하여 설정 (예시)\n",
    "    short_period = max(5, int(window / 4))\n",
    "    long_period = max(short_period + 5, int(window / 2))\n",
    "    signal_period = max(3, int(window / 8))\n",
    "    data_1m, macd_cols = calculate_macd(data_1m, short_period, long_period, signal_period)\n",
    "    features_1m[f'MACD_{window}'] = macd_cols\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    k_period = max(5, int(window / 4))\n",
    "    d_period = max(3, int(window / 8))\n",
    "    data_1m, stoch_cols = calculate_stochastic_oscillator(data_1m, window, k_period, extra_str=f'_{window}')\n",
    "    features_1m[f'STOCH_{window}'] = stoch_cols\n",
    "\n",
    "    # ADX\n",
    "    data_1m, adx_cols = calculate_adx(data_1m, window)\n",
    "    features_1m[f'ADX_{window}'] = adx_cols\n",
    "\n",
    "    # ATR\n",
    "    data_1m, atr_cols = calculate_atr(data_1m, window)\n",
    "    features_1m[f'ATR_{window}'] = atr_cols\n",
    "\n",
    "    # OBV\n",
    "    data_1m, obv_cols = calculate_obv(data_1m)\n",
    "    features_1m[f'OBV_{window}'] = obv_cols\n",
    "\n",
    "    # Williams %R\n",
    "    data_1m, will_cols = calculate_williams_r(data_1m, window)\n",
    "    features_1m[f'WILL_%R_{window}'] = will_cols\n",
    "\n",
    "    # Support & Resistance (예시)\n",
    "    data_1m, sr_col = calculate_support_resistance_numba(data_1m, window=window)\n",
    "    features_1m[f'SR_{window}'] = sr_col\n",
    "\n",
    "    # Base Feature (예시: alpha 값을 window에 따라 조정)\n",
    "    alpha_val = window * 5\n",
    "    data_1m, base_feature = base_feature_fn(data_1m, alpha=alpha_val)\n",
    "    features_1m[f'BASE_{window}'] = base_feature\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1일봉 데이터: 다양한 윈도우 (예: 20, 60, 120봉)를 사용하여 feature 생성\n",
    "time_windows_1d = [20, 60, 120]\n",
    "features_1d = {}\n",
    "\n",
    "for window in time_windows_1d:\n",
    "    # 이동평균 (EMA)\n",
    "    data_1d, ma_cols_1d, ma_cols_rel_1d = calculate_MA_data(data_1d, window, 'EMA', extra_str=f'_{window}')\n",
    "    features_1d[f'MA_{window}'] = ma_cols_rel_1d\n",
    "\n",
    "    # EMA Bollinger Bands\n",
    "    data_1d, bb_cols_1d, bb_cols_rel_1d = calculate_ema_bollinger_bands(data_1d, window, extra_str=f'_{window}')\n",
    "    features_1d[f'BB_{window}'] = bb_cols_rel_1d\n",
    "\n",
    "    # RSI (윈도우의 1/3 또는 최소 5)\n",
    "    rsi_window = max(5, int(window / 3))\n",
    "    data_1d, rsi_cols_1d = calculate_rsi(data_1d, window=rsi_window, extra_str=f'_{window}')\n",
    "    features_1d[f'RSI_{window}'] = rsi_cols_1d\n",
    "\n",
    "    # MACD\n",
    "    short_period = max(5, int(window / 4))\n",
    "    long_period = max(short_period + 5, int(window / 2))\n",
    "    signal_period = max(3, int(window / 8))\n",
    "    data_1d, macd_cols_1d = calculate_macd(data_1d, short_period, long_period, signal_period, extra_str=f'_{window}')\n",
    "    features_1d[f'MACD_{window}'] = macd_cols_1d\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    k_period = max(5, int(window / 4))\n",
    "    d_period = max(3, int(window / 8))\n",
    "    data_1d, stoch_cols_1d = calculate_stochastic_oscillator(data_1d, window, k_period, extra_str=f'_{window}')\n",
    "    features_1d[f'STOCH_{window}'] = stoch_cols_1d\n",
    "\n",
    "    # ADX\n",
    "    data_1d, adx_cols_1d = calculate_adx(data_1d, window, extra_str=f'_{window}')\n",
    "    features_1d[f'ADX_{window}'] = adx_cols_1d\n",
    "\n",
    "    # ATR\n",
    "    data_1d, atr_cols_1d = calculate_atr(data_1d, window, extra_str=f'_{window}')\n",
    "    features_1d[f'ATR_{window}'] = atr_cols_1d\n",
    "\n",
    "    # OBV\n",
    "    data_1d, obv_cols_1d = calculate_obv(data_1d, extra_str=f'_{window}')\n",
    "    features_1d[f'OBV_{window}'] = obv_cols_1d\n",
    "\n",
    "    # Williams %R\n",
    "    data_1d, will_cols_1d = calculate_williams_r(data_1d, window, extra_str=f'_{window}')\n",
    "    features_1d[f'WILL_%R_{window}'] = will_cols_1d\n",
    "\n",
    "    # Support & Resistance\n",
    "    data_1d, sr_col_1d = calculate_support_resistance_numba(data_1d, window=window, extra_str=f'_{window}')\n",
    "    features_1d[f'SR_{window}'] = sr_col_1d\n",
    "\n",
    "    # Base Feature (예시: alpha 값을 window에 따라 조정)\n",
    "    alpha_val = window * 2\n",
    "    data_1d, base_feature_1d = base_feature_fn(data_1d, extra_str=f'_{window}', alpha=alpha_val)\n",
    "    features_1d[f'BASE_{window}'] = base_feature_1d\n",
    "\n",
    "# -------------------------------------------\n",
    "# 최종 Feature Column 구성\n",
    "# 사용하지 않을 컬럼 (드롭할 컬럼) 지정\n",
    "drop_column = [\n",
    "    'Open time', 'Close time', 'Quote asset volume', 'Ignore',\n",
    "    'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume'\n",
    "]\n",
    "\n",
    "# 테스트용 컬럼 (예시)\n",
    "test_column = ['Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume']\n",
    "\n",
    "# 1분봉 데이터 최종 feature column 구성: 기존 test_column과 각 indicator별 컬럼 리스트를 합산\n",
    "final_feature_columns_1m = test_column + sum([features_1m[key] for key in features_1m.keys()], [])\n",
    "\n",
    "# 1일봉 데이터 최종 feature column 구성\n",
    "final_feature_columns_1d = test_column + sum([features_1d[key] for key in features_1d.keys()], [])\n",
    "\n",
    "# 이제 data_1m, data_1d와 각각의 final_feature_columns를 모델 입력 등으로 활용할 수 있습니다.\n",
    "\n",
    "\n",
    "# # bb_entry_pos_list, patience_list, bb_entry_index_list = BB_fitness_fn(data_1m)\n",
    "# bb_macd_entry_pos_list, patience_list, bb_macd_entry_index_list = BB_MACD_fitness_fn(data_1m, 240, 60, 180)\n",
    "# # bb_macd_entry_pos_list, patience_list, bb_macd_entry_index_list = BB_MACD_fitness_fn(data_1m, 60, 20, 60)\n",
    "\n",
    "# # Prepare Dataset\n",
    "# data_tensor = make_dataset(\n",
    "#     data_1m, data_1d,\n",
    "#     using_column=feature_column, using_column_1d=feature_column_1d,\n",
    "#     window_size=600, window_size_1d=60,\n",
    "#     entry_pos_list=bb_macd_entry_pos_list, patience_list=patience_list,\n",
    "#     use_1d_data=True\n",
    "# )\n",
    "# entry_pos_list = np.array(bb_macd_entry_pos_list)[np.array(bb_macd_entry_pos_list) != 'hold']\n",
    "\n",
    "# dataset_1m = []\n",
    "# dataset_1d = []\n",
    "# skip_data_cnt = 0\n",
    "# for data in data_tensor:\n",
    "#     if len(data[0]) == 600 and len(data[1]) == 60:\n",
    "#         dataset_1m.append(torch.from_numpy(data[0]).unsqueeze(dim=0))\n",
    "#         dataset_1d.append(torch.from_numpy(data[1]).unsqueeze(dim=0))\n",
    "#     else:\n",
    "#         skip_data_cnt += 1\n",
    "# dataset_1m = torch.cat(dataset_1m, dim=0)\n",
    "# dataset_1d = torch.cat(dataset_1d, dim=0)\n",
    "\n",
    "# # Avoid division by zero by replacing zero denominators with a small epsilon value\n",
    "# epsilon = 1e-10\n",
    "# dataset_1m[:, :, :4] = dataset_1m[:, :, :4] / (torch.mean(dataset_1m[:, :, :4], dim=1).unsqueeze(dim=1) + epsilon)\n",
    "# dataset_1d[:, :, :4] = dataset_1d[:, :, :4] / (torch.mean(dataset_1d[:, :, :4], dim=1).unsqueeze(dim=1) + epsilon)\n",
    "\n",
    "# dataset_1m = replace_nan_with_zero(dataset_1m)\n",
    "# dataset_1d = replace_nan_with_zero(dataset_1d)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# data_to_save = {\n",
    "#     'dataset_1m': dataset_1m,\n",
    "#     'dataset_1d': dataset_1d,\n",
    "#     'skip_data_cnt': skip_data_cnt,\n",
    "#     'entry_pos_list': entry_pos_list,\n",
    "#     'bb_macd_entry_pos_list': bb_macd_entry_pos_list,\n",
    "#     'bb_macd_entry_index_list': bb_macd_entry_index_list\n",
    "# }\n",
    "\n",
    "# with open('/root/daily/bit_4/backup_feature_data/data.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the file into separate variables\n",
    "with open('/root/daily/bit_4/backup_feature_data/data.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "dataset_1m = loaded_data['dataset_1m']\n",
    "dataset_1d = loaded_data['dataset_1d']\n",
    "skip_data_cnt = loaded_data['skip_data_cnt']\n",
    "entry_pos_list = loaded_data['entry_pos_list']\n",
    "bb_macd_entry_pos_list = loaded_data['bb_macd_entry_pos_list']\n",
    "bb_macd_entry_index_list = loaded_data['bb_macd_entry_index_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_skip_data_cnt = int(len(dataset_1m)*0.6) + skip_data_cnt\n",
    "test_skip_data_cnt = int(len(dataset_1m)*0.8) + skip_data_cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 173950000\n",
      "generation  0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [05:54<00:00, 16.87s/it]\n",
      " 61%|██████    | 25725/42211 [03:17<02:06, 130.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:11<03:19, 82.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:12<03:20, 82.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:15<00:00, 17.87s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.40s/it]\n",
      " 61%|██████    | 25725/42211 [05:12<03:20, 82.38it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.13it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:01<03:13, 85.27it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.21it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:11<03:19, 82.56it/s]  \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 438.61it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.26it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 444.84it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:13<03:20, 82.12it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.29it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:03<00:00, 17.33s/it]\n",
      " 61%|██████    | 25725/42211 [05:13<03:21, 81.97it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 446.63it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [05:13<03:20, 82.15it/s]  \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.47it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [04:58<03:11, 86.27it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.56it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.40it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.00it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:03<00:00, 17.33s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.13it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.88it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.35s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.00it/s] \n",
      " 80%|████████  | 33968/42211 [01:24<00:20, 402.23it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:07<03:17, 83.55it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.27it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:05<03:15, 84.20it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.82it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [05:04<03:15, 84.37it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.98it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.15it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.10it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [05:05<03:15, 84.33it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.94it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:19, 82.79it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.22it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:07<03:17, 83.63it/s] \n",
      " 80%|████████  | 33968/42211 [01:27<00:21, 387.39it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [04:57<03:10, 86.49it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.88it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [04:54<03:08, 87.28it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.96it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:07<03:17, 83.64it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.95it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.40s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.03it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.29it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:04<03:15, 84.53it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.26it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.38it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.00it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:15<03:22, 81.58it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.30it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:11<03:19, 82.56it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.10it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  31: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:01<03:13, 85.22it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 444.86it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  32: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:03<03:14, 84.85it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 446.55it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  33: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:04<03:15, 84.52it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 444.62it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  34: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.38s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:19, 82.79it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 440.10it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  35: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.36s/it]\n",
      " 61%|██████    | 25725/42211 [04:58<03:11, 86.27it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.62it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  36: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.40s/it]\n",
      " 61%|██████    | 25725/42211 [05:06<03:16, 83.87it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.92it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  37: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.41s/it]\n",
      " 61%|██████    | 25725/42211 [05:04<03:15, 84.38it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.52it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  38: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:04<03:15, 84.36it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.00it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  39: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:04<00:00, 17.37s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.16it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.36it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  40: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.02it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.57it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  41: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.41s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:18, 82.96it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.33it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  42: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:18, 82.87it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.27it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  43: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [04:47<03:04, 89.59it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 440.63it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  44: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:24<03:28, 79.24it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.96it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  45: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.43s/it]\n",
      " 61%|██████    | 25725/42211 [04:59<03:11, 86.02it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.23it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  46: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:06<03:16, 83.89it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 440.26it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  47: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:06<03:16, 84.05it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.91it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  48: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.41s/it]\n",
      " 61%|██████    | 25725/42211 [05:32<03:32, 77.43it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.23it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  49: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.43s/it]\n",
      " 61%|██████    | 25725/42211 [05:06<03:16, 83.80it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 440.58it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 173950000\n",
      "generation  0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:03<00:00, 17.31s/it]\n",
      " 61%|██████    | 25725/42211 [03:10<02:02, 134.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.44s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.44s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:18, 82.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.45s/it]\n",
      " 61%|██████    | 25725/42211 [05:03<03:14, 84.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.42s/it]\n",
      " 61%|██████    | 25725/42211 [05:27<03:30, 78.44it/s]  \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.33it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.44s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.27it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 443.46it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.40s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.46it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.22it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.46s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.40it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.29it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.45s/it]\n",
      " 61%|██████    | 25725/42211 [05:09<03:18, 83.14it/s] \n",
      " 80%|████████  | 33968/42211 [01:18<00:18, 434.21it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.43s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.51it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 439.00it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.41s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.38it/s] \n",
      " 80%|████████  | 33968/42211 [01:17<00:18, 438.28it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.44s/it]\n",
      " 61%|██████    | 25725/42211 [05:03<03:14, 84.66it/s]  \n",
      " 80%|████████  | 33968/42211 [01:15<00:18, 448.12it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.44s/it]\n",
      " 61%|██████    | 25725/42211 [05:11<03:19, 82.51it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 441.57it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:07<00:00, 17.51s/it]\n",
      " 61%|██████    | 25725/42211 [05:08<03:17, 83.41it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 445.68it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:05<00:00, 17.39s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:18, 82.89it/s] \n",
      " 80%|████████  | 33968/42211 [01:16<00:18, 442.64it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:07<00:00, 17.48s/it]\n",
      " 61%|██████    | 25725/42211 [05:10<03:18, 82.93it/s] \n",
      " 80%|████████  | 33968/42211 [01:15<00:18, 447.15it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation  17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 21/21 [06:06<00:00, 17.47s/it]\n",
      " 35%|███▌      | 14875/42211 [02:50<05:49, 78.18it/s] "
     ]
    }
   ],
   "source": [
    "# Evolution Setup\n",
    "# 전역적으로 기울기 계산 비활성화\n",
    "torch.set_grad_enabled(False)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "chromosomes_size=10000\n",
    "window_size=240\n",
    "EPOCH = 8\n",
    "gen_loop=50\n",
    "best_size=10000\n",
    "elite_size=2000\n",
    "profit_init=1\n",
    "device = 'cuda:1'\n",
    "group = 10000\n",
    "start_gen = 0\n",
    "best_profit = None\n",
    "best_chromosomes = None\n",
    "\n",
    "# state_dict_path = '/root/daily/bit_4/generation/generation_27.pt'\n",
    "# if os.path.exists(state_dict_path):\n",
    "#     state_dict = torch.load(state_dict_path)\n",
    "#     start_gen = state_dict['generation'] + 1\n",
    "#     best_profit = state_dict['best_profit']\n",
    "#     best_chromosomes = state_dict['best_chromosomes']\n",
    "    # prescriptor.load_state_dict(state_dict['prescriptor_state_dict'],strict=True)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    prescriptor = Prescriptor(\n",
    "        basic_block=CryptoModelTCN, \n",
    "        small_input_dim=25, \n",
    "        large_input_dim=25,\n",
    "        fc_hidden_size=16,\n",
    "        small_lstm_hidden_dim=16,\n",
    "        large_lstm_hidden_dim=16,\n",
    "        output_dim=8, \n",
    "        after_input_dim=11, \n",
    "        after_hidden_dim=16, \n",
    "        after_output_dim=5, \n",
    "        num_blocks=group,\n",
    "    ).to(device).eval()\n",
    "\n",
    "    # if i == 1:\n",
    "    #     start_gen=0\n",
    "\n",
    "    # if i == 0:\n",
    "    #     prescriptor.load_state_dict(state_dict['prescriptor_state_dict'],strict=True)\n",
    "\n",
    "    total_param = sum(p.numel() for p in prescriptor.parameters())\n",
    "    print(f\"Total parameters: {total_param}\")\n",
    "\n",
    "    selection = RouletteSelection(elite_num=2000, parents_num=2000, minimize=False)\n",
    "    # selection = ParetoLexsortSelection(elite_num=2000, parents_num=4000,\n",
    "    #                                     priority=[], prior_ratio= [],\n",
    "    #                                     prob_method= 'softmax',minimize=False)\n",
    "    # crossover = DifferentialEvolutionOperator()\n",
    "    # crossover = UniformCrossover(num_parents=4)\n",
    "    # crossover = CenDE_DOBLOperator()\n",
    "    mutation = ChainMutation([RandomValueMutation(mut_prob=0.05), AddUniformMutation(mut_prob=0.1)])\n",
    "    crossover = UniformCrossover(num_parents=1)\n",
    "    # mutation = AddNormalMutation(mut_prob=0.1)\n",
    "    evolution = Evolution(\n",
    "        prescriptor=prescriptor,\n",
    "        selection=selection,\n",
    "        crossover=crossover,\n",
    "        mutation=mutation\n",
    "    )\n",
    "\n",
    "    best_chromosomes, best_profit = generation_valid(\n",
    "        data_1m=data_1m,\n",
    "        dataset_1m=dataset_1m,\n",
    "        dataset_1d=dataset_1d,\n",
    "        prescriptor=prescriptor,\n",
    "        evolution=evolution,\n",
    "        skip_data_cnt=skip_data_cnt,\n",
    "        valid_skip_data_cnt=valid_skip_data_cnt,\n",
    "        test_skip_data_cnt=test_skip_data_cnt,\n",
    "        chromosomes_size=chromosomes_size,\n",
    "        window_size=window_size,\n",
    "        gen_loop=gen_loop,\n",
    "        best_size=best_size,\n",
    "        elite_size=elite_size,\n",
    "        profit_init=profit_init,\n",
    "        entry_index_list=bb_macd_entry_index_list,\n",
    "        entry_pos_list=entry_pos_list,\n",
    "        best_profit=best_profit,\n",
    "        best_chromosomes=best_chromosomes,\n",
    "        start_gen=start_gen,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
