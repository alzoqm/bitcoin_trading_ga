{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from data_loader import load_data_1m\n",
    "from feature_calculations import (\n",
    "    resample_data, calculate_MA_data, calculate_ema_bollinger_bands, calculate_rsi,\n",
    "    calculate_macd, calculate_stochastic_oscillator, calculate_adx, calculate_atr,\n",
    "    calculate_obv, calculate_williams_r, base_feature_fn, cyclic_encode_fn, log_transform\n",
    ")\n",
    "from strategies import BB_fitness_fn, BB_MACD_fitness_fn\n",
    "from dataset import make_dataset, replace_nan_with_zero\n",
    "from train_functions_rl import inference, fitness_fn, generation_valid, generation_test\n",
    "\n",
    "from Prescriptor import Prescriptor, ChromosomeSelectorModel\n",
    "from Evolution.crossover import UniformCrossover, WeightedSumCrossover, DifferentialEvolutionOperator, CenDE_DOBLOperator\n",
    "from Evolution.mutation import MultiplyNormalMutation, MultiplyUniformMutation, AddNormalMutation, AddUniformMutation, ChainMutation, FlipSignMutation\n",
    "from Evolution.mutation import RandomValueMutation\n",
    "from Evolution.selection import RouletteSelection, TournamentSelection, ParetoLexsortSelection\n",
    "from Evolution import Evolution\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bisect\n",
    "\n",
    "class RLDataset(Dataset):\n",
    "    def __init__(self, data_1m: pd.DataFrame, data_1d: pd.DataFrame, feature_columns: list, close_time_list: list, index_list: list):\n",
    "        \"\"\"\n",
    "        초기화 메서드\n",
    "        \"\"\"\n",
    "        self.data_1m = data_1m\n",
    "        self.data_1d = data_1d\n",
    "        self.feature_columns = feature_columns\n",
    "        self.close_time_list = close_time_list\n",
    "        self.index_list = sorted(index_list)  # 정렬된 리스트로 가정\n",
    "        \n",
    "        # data_1m의 'Close time'을 numpy array로 변환하여 searchsorted 사용\n",
    "        self.data_1m_times = self.data_1m['Close time'].values\n",
    "        self.index_array = sorted(self.index_list)  # 이진 탐색을 위해 정렬\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 전체 길이를 반환합니다.\n",
    "        \"\"\"\n",
    "        return len(self.close_time_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 데이터를 반환합니다.\n",
    "        \"\"\"\n",
    "        # 1. close_time_list에서 해당 인덱스의 날짜를 가져옵니다.\n",
    "        current_date = self.close_time_list[idx]\n",
    "        \n",
    "        # 2. data_1d에서 현재 날짜를 포함한 60일 이전의 데이터를 가져옵니다.\n",
    "        start_date = current_date - pd.Timedelta(days=59)  # 현재 날짜 포함 60일\n",
    "        mask = (self.data_1d['Close time'] >= start_date) & (self.data_1d['Close time'] <= current_date)\n",
    "        data_60d = self.data_1d.loc[mask, self.feature_columns]\n",
    "        \n",
    "        # 데이터가 충분하지 않으면 예외 처리\n",
    "        if len(data_60d) < 60:\n",
    "            raise ValueError(f\"Not enough data for index {idx}: expected 60 days, got {len(data_60d)} days.\")\n",
    "        \n",
    "        # 피처를 텐서로 변환\n",
    "        features = torch.tensor(data_60d.values, dtype=torch.float32)\n",
    "        \n",
    "        # 3. 다음 날짜의 시작 인덱스를 data_1m에서 찾습니다.\n",
    "        next_date = current_date + pd.Timedelta(days=1)\n",
    "        \n",
    "        # 이진 탐색을 사용하여 next_date 이상의 첫 번째 위치 찾기\n",
    "        start_pos = bisect.bisect_left(self.data_1m_times, next_date)\n",
    "        if start_pos >= len(self.data_1m_times):\n",
    "            raise ValueError(f\"No data found in data_1m after {next_date}.\")\n",
    "        start_index = self.data_1m.index[start_pos]\n",
    "        \n",
    "        # 4. index_list에서 start_index보다 큰 첫 번째 값을 찾습니다.\n",
    "        # 이진 탐색을 사용하여 start_index보다 큰 첫 번째 값 찾기\n",
    "        greater_pos = bisect.bisect_right(self.index_array, start_index)\n",
    "        if greater_pos >= len(self.index_array):\n",
    "            raise ValueError(f\"No index in index_list greater than start_index {start_index}.\")\n",
    "        first_greater_index = self.index_array[greater_pos]\n",
    "        \n",
    "        # 5. 다음 날짜 + 30일의 날짜를 계산하고, 그 날짜의 마지막 인덱스를 찾습니다.\n",
    "        end_date = current_date + pd.Timedelta(days=31)  # 다음날 포함 30일\n",
    "        # 이진 탐색을 사용하여 end_date 이하의 마지막 위치 찾기\n",
    "        end_pos = bisect.bisect_right(self.data_1m_times, end_date) - 1\n",
    "        if end_pos < 0:\n",
    "            end_index = self.index_array[-1]\n",
    "        else:\n",
    "            end_index_candidate = self.data_1m.index[end_pos]\n",
    "            # index_list에서 end_index_candidate보다 작은 가장 큰 값 찾기\n",
    "            less_pos = bisect.bisect_left(self.index_array, end_index_candidate)\n",
    "            if less_pos == 0:\n",
    "                raise ValueError(f\"No index in index_list less than end_index_candidate {end_index_candidate}.\")\n",
    "            end_index = self.index_array[less_pos - 1]\n",
    "        \n",
    "        # 6. 추출한 데이터 프레임, 시작 index, end_index를 반환합니다.\n",
    "        return features, first_greater_index, end_index, greater_pos, less_pos - 1\n",
    "    \n",
    "def data_setting():\n",
    "    # Load Data\n",
    "    data_1m = load_data_1m('/root/daily/bit/data/1min_bitusdt.pkl')\n",
    "    data_1m = data_1m.iloc[:500000]\n",
    "\n",
    "    # Resample data to 1D\n",
    "    data_1d = resample_data(data_1m, '1D')\n",
    "    data_1d['Close time'] = data_1d.index\n",
    "    data_1d = data_1d.reset_index(drop=True)\n",
    "\n",
    "    # Apply Feature Calculations\n",
    "    # For 1D Data\n",
    "    data_1d, ma_cols_1d, ma_cols_rel_1d = calculate_MA_data(data_1d, 60, 'EMA', '_1d')\n",
    "    data_1d, bb_cols_1d, bb_cols_rel_1d = calculate_ema_bollinger_bands(data_1d, 60, extra_str='_1d')\n",
    "    data_1d, rsi_cols_1d = calculate_rsi(data_1d, window=20, extra_str='_1d')\n",
    "    data_1d, macd_cols_1d = calculate_macd(data_1d, 20, 120, 60, extra_str='_1d')\n",
    "    data_1d, stoch_cols_1d = calculate_stochastic_oscillator(data_1d, 60, 20, extra_str='_1d')\n",
    "    data_1d, adx_cols_1d = calculate_adx(data_1d, 60, extra_str='_1d')\n",
    "    data_1d, atr_cols_1d = calculate_atr(data_1d, 60, extra_str='_1d')\n",
    "    data_1d, obv_cols_1d = calculate_obv(data_1d, extra_str='_1d')\n",
    "    data_1d, will_cols_1d = calculate_williams_r(data_1d, 60, extra_str='_1d')\n",
    "    data_1d, base_feature_1d = base_feature_fn(data_1d, extra_str='_1d')\n",
    "    data_1d, cyclice_encoding_1d = cyclic_encode_fn(data_1d, 'Close time', 'day_of_year')\n",
    "\n",
    "    # For 1M Data\n",
    "    data_1m, ma_cols, ma_cols_rel = calculate_MA_data(data_1m, 240, 'EMA')\n",
    "    data_1m, bb_cols, bb_cols_rel = calculate_ema_bollinger_bands(data_1m, 240)\n",
    "    data_1m, rsi_cols = calculate_rsi(data_1m, window=60)\n",
    "    data_1m, macd_cols = calculate_macd(data_1m, 60, 600, 240)\n",
    "    data_1m, stoch_cols = calculate_stochastic_oscillator(data_1m, 240, 60)\n",
    "    data_1m, adx_cols = calculate_adx(data_1m, 240)\n",
    "    data_1m, atr_cols = calculate_atr(data_1m, 240)\n",
    "    data_1m, obv_cols = calculate_obv(data_1m)\n",
    "    data_1m, will_cols = calculate_williams_r(data_1m, 240)\n",
    "    data_1m, base_feature = base_feature_fn(data_1m)\n",
    "    data_1m, cyclice_encoding = cyclic_encode_fn(data_1m, 'Open time')\n",
    "\n",
    "    data_1m, short_ma_cols, short_ma_cols_rel = calculate_MA_data(data_1m, 60, 'EMA')\n",
    "    data_1m, long_ma_cols, long_ma_cols_rel = calculate_MA_data(data_1m, 180, 'EMA')\n",
    "\n",
    "    # Prepare Feature Columns\n",
    "    drop_column = [\n",
    "        'Open time', 'Close time', 'Quote asset volume', 'Ignore',\n",
    "        'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume'\n",
    "    ]\n",
    "    feature_column = (\n",
    "        ma_cols_rel + bb_cols_rel + rsi_cols + macd_cols + stoch_cols +\n",
    "        adx_cols + will_cols + base_feature + cyclice_encoding  # Excluding obv and atr\n",
    "    )\n",
    "    feature_column_1d = (\n",
    "        ma_cols_rel_1d + bb_cols_rel_1d + rsi_cols_1d + macd_cols_1d + stoch_cols_1d +\n",
    "        adx_cols_1d + will_cols_1d + base_feature_1d + cyclice_encoding_1d\n",
    "    )\n",
    "\n",
    "    # Apply Log Transform\n",
    "    for feature in feature_column:\n",
    "        data_1m[feature] = log_transform(data_1m[feature])\n",
    "\n",
    "    for feature in feature_column_1d:\n",
    "        data_1d[feature] = log_transform(data_1d[feature])\n",
    "\n",
    "    data_1d['%D_20__1d'] = 0\n",
    "    data_1d['ADX_60__1d'] = 0\n",
    "\n",
    "    # bb_entry_pos_list, patience_list, bb_entry_index_list = BB_fitness_fn(data_1m)\n",
    "    bb_macd_entry_pos_list, patience_list, bb_macd_entry_index_list = BB_MACD_fitness_fn(data_1m, 240, 60, 180)\n",
    "\n",
    "    # Prepare Dataset\n",
    "    data_tensor = make_dataset(\n",
    "        data_1m, data_1d,\n",
    "        using_column=feature_column, using_column_1d=feature_column_1d,\n",
    "        window_size=240, window_size_1d=60,\n",
    "        entry_pos_list=bb_macd_entry_pos_list, patience_list=patience_list,\n",
    "        use_1d_data=True\n",
    "    )\n",
    "    entry_pos_list = np.array(bb_macd_entry_pos_list)[np.array(bb_macd_entry_pos_list) != 'hold']\n",
    "\n",
    "    dataset_1m = []\n",
    "    dataset_1d = []\n",
    "    skip_data_cnt = 0\n",
    "    for data in data_tensor:\n",
    "        if len(data[0]) == 240 and len(data[1]) == 60:\n",
    "            dataset_1m.append(torch.from_numpy(data[0]).unsqueeze(dim=0))\n",
    "            dataset_1d.append(torch.from_numpy(data[1]).unsqueeze(dim=0))\n",
    "        else:\n",
    "            skip_data_cnt += 1\n",
    "    dataset_1m = torch.cat(dataset_1m, dim=0)\n",
    "    dataset_1d = torch.cat(dataset_1d, dim=0)\n",
    "    dataset_1m = replace_nan_with_zero(dataset_1m)\n",
    "    dataset_1d = replace_nan_with_zero(dataset_1d)\n",
    "\n",
    "    return (data_1m, data_1d, dataset_1m, dataset_1d, skip_data_cnt, \n",
    "            entry_pos_list, bb_macd_entry_index_list, feature_column, \n",
    "            feature_column_1d)\n",
    "\n",
    "\n",
    "data_1m, data_1d, dataset_1m, dataset_1d, skip_data_cnt, entry_pos_list, \\\n",
    "bb_macd_entry_index_list, feature_column, feature_column_1d = data_setting()\n",
    "\n",
    "close_time_list = data_1d['Close time'].tolist()[60:-30]\n",
    "dataset_rl = RLDataset(data_1m, data_1d, feature_column_1d, close_time_list, bb_macd_entry_index_list)\n",
    "# dataloader_rl = DataLoader(dataset_rl, batch_size=32, shuffle=False)\n",
    "\n",
    "def setting_model(best_index=None):\n",
    "    torch.set_grad_enabled(False)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    state_dict_path = '/root/daily/bit/generation/generation_39.pt'\n",
    "    if os.path.exists(state_dict_path):\n",
    "        state_dict = torch.load(state_dict_path)\n",
    "        start_gen = state_dict['generation'] + 1\n",
    "        best_profit = state_dict['best_profit']\n",
    "        best_chromosomes = state_dict['best_chromosomes']\n",
    "\n",
    "    if best_index is None:\n",
    "        best_index = [i for i in range(len(best_profit))]\n",
    "\n",
    "    chromosomes_size = len(best_index)\n",
    "\n",
    "    # Evolution Setup\n",
    "    device = 'cuda:0'\n",
    "    group = len(best_index)\n",
    "    prescriptor = Prescriptor(\n",
    "        basic_block=None, \n",
    "        base_small_input_dim=20, \n",
    "        base_large_input_dim=20,\n",
    "        base_hidden_dim=32, \n",
    "        base_output_dim=16, \n",
    "        after_input_dim=19, \n",
    "        after_hidden_dim=32, \n",
    "        after_output_dim=6, \n",
    "        num_blocks=len(best_index),\n",
    "    ).to(device)\n",
    "\n",
    "    total_param = sum(p.numel() for p in prescriptor.parameters())\n",
    "    print(f\"Total parameters: {total_param}\")\n",
    "\n",
    "    selection = RouletteSelection(elite_num=2000, parents_num=4000, minimize=False)\n",
    "    crossover = DifferentialEvolutionOperator()\n",
    "    mutation = RandomValueMutation(mut_prob=0.05)\n",
    "    evolution = Evolution(\n",
    "        prescriptor=prescriptor,\n",
    "        selection=selection,\n",
    "        crossover=crossover,\n",
    "        mutation=mutation\n",
    "    )\n",
    "\n",
    "    best_chromosomes = best_chromosomes[best_index]\n",
    "    init_chromosomes, base_ch_shape, after_ch_shape, device = evolution.flatten_chromosomes()\n",
    "    device = 'cuda:0'\n",
    "    evolution.update_chromosomes(best_chromosomes, base_ch_shape, after_ch_shape, device)\n",
    "\n",
    "    return evolution, prescriptor, chromosomes_size, device\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, data_1d):\n",
    "        self.data = data\n",
    "        self.data_1d = data_1d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.data_1d[idx]\n",
    "\n",
    "def evo_inference(dataset_1m, dataset_1d, prescriptor, device):\n",
    "    def inference(scaled_tensor, scaled_tensor_1d, model, device='cuda:0'):\n",
    "        dataset = CustomDataset(scaled_tensor, scaled_tensor_1d)\n",
    "        dataloader = DataLoader(dataset, batch_size=2048, shuffle=False, num_workers=8, pin_memory=True)\n",
    "        logits = []\n",
    "        for data, data_1d in dataloader:\n",
    "            data = data.to(torch.float32).to(device)\n",
    "            data_1d = data_1d.to(torch.float32).to(device)\n",
    "            logit = model.base_forward(data, data_1d)\n",
    "            logits.append(logit)\n",
    "        return logits\n",
    "    probs = inference(dataset_1m, dataset_1d, prescriptor, device)\n",
    "    probs = torch.concat(probs, dim=1)\n",
    "    probs = probs.squeeze(dim=2)\n",
    "\n",
    "    return probs\n",
    "\n",
    "evolution, prescriptor, chromosomes_size, device = setting_model()\n",
    "rl_model = ChromosomeSelectorModel(20, 256, chromosomes_size, 4).to(device)\n",
    "probs = evo_inference(dataset_1m, dataset_1d, prescriptor, device)\n",
    "\n",
    "def train_rl(rl_model, dataset_rl, prescriptor, data_1m, probs, entry_index_list, entry_pos_list, skip_data_cnt, chromosomes_size,\n",
    "             device):\n",
    "    \n",
    "        for data in dataset_rl:\n",
    "            # rl train\n",
    "            # rl action is chromosome size naming is rl_weight\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # output shape: (chromosome size, 7)\n",
    "                train_metrics = fitness_fn(\n",
    "                    prescriptor=prescriptor,\n",
    "                    data=data_1m,\n",
    "                    probs=probs,\n",
    "                    entry_index_list=entry_index_list,\n",
    "                    entry_pos_list=entry_pos_list,\n",
    "                    skip_data_cnt=skip_data_cnt,\n",
    "                    start_data_cnt=skip_data_cnt,\n",
    "                    chromosomes_size=chromosomes_size,\n",
    "                    window_size=240,\n",
    "                    alpha=1,\n",
    "                    cut_percent=90,\n",
    "                    device=device,\n",
    "                    stop_cnt=skip_data_cnt,\n",
    "                    profit_init=1,\n",
    "                    limit=4\n",
    "                )\n",
    "\n",
    "\n",
    "                base_metric = torch.mean(train_metrics[:, 6])\n",
    "                rl_metric = torch.mean(train_metrics[:, 6] * rl_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:55<00:00, 8993.41it/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3005456\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92, 5027, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
